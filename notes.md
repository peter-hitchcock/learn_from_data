# ch 1 
- running example: taking predictors of credit to make y/n approval decisions  
- we denote the ideal fx $f:X \rightarrow Y$ that does the best possible; in practice we'll estimate $g:X \rightarrow  Y$ to approximate f  
- a simple model following this setup  
    
    - let $\R^d$ be the d-dim Euclidean input space $Y \in [ -1, 1]$ the output space (yes/no decisions)  
    $H$ is the hypothesis set $\forall h \in H$ and w a fxal form $h(x)$ giving weights to coords of x  
    
    - then approve credit is $\sum{_1^d} x_i w_i$ > threshold, deny if $\sum{_1^d} x_i w_i$ <  threshold  

    - more compact: $h(x) = sign((\sum{_1^d} x_i w_i) + b)$  

        - where $sign(s) = 1 \ if s >0$, 0 otherwise 
        - and $b$ is bias that determines the threshold bc credit is approved if 
        $\sum{_1^d} x_i w_i > - b$  
    - this is a perceptron. we use the data to determine weights + threshold. negative weights are for indicators bad on credit. b will dictate how lenient or stringent we are.  

$E_{in}(h)$ is fraction of D where f (fx) and h(ypothesis) disagree, calculated as the proportion of disagreements      

# ch2  
generalization error = diff between $E_{in}$ and $E_{out}$, hoeffding is one way to quantify this. does so via probabilistic bound. can think of it as  
- pick a tolerance delta like .05  
- say with p = 1 - delta that 
$$E_{out}(g) \leq E_{in}(g) + \sqrt{{ \frac{1}{2N} \ ln \ \frac{2M}{\delta}}}$$

where M is the size of hypothesis set  

can think of $\sqrt{{ \frac{1}{2N} \ ln \ \frac{2M}{\delta}}}$ as some $\epsilon$ added to $E_{in}$  

- most interesting situations have unlimited hypotheses. so we need to replace our M and we can do this with $m_H$ - the maximum # of dichotomies can be generated by H on N points - in this starting place with N points and binary target fx  
    - where we end up is that any growth fx w break point is bounded by polynomial 

$$E_{out}(g) \leq[?]  \ \ E_{in}(g) + \sqrt{{ \frac{1}{2N} \ ln \ \frac{2m_h}{\delta}}}$$  

- except when $d_{VC}$ is $\infty$ -- those are the class of bad models where there aren't clear generalizability conclusions  

- this actually is an oversimplification but general idea's right  

- big picture the VC gives some of the most general routes we have and is good for a loose estimate but in a real application we'll use a test set to forecast $E_out$ more precisely  

## bias - variance tradeoff  
- amounts to another approach to generalization; rather than bound error on E_out we'll decompose it into bias + variance 
- bias is $(\bar{g}(x) - f(x))^2$; in words the estimated fx's discrep from frue fx and the average is over possible data sets   


