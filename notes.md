# ch 1 
- running example: taking predictors of credit to make y/n approval decisions  
- we denote the ideal fx $f:X \rightarrow Y$ that does the best possible; in practice we'll estimate $g:X \rightarrow  Y$ to approximate f  
- a simple model following this setup  
    
    - let $\R^d$ be the d-dim Euclidean input space $Y \in [ -1, 1]$ the output space (yes/no decisions)  
    $\mathcal{H}$ is the hypothesis set $\forall h \in \mathcal{H}$ and w a fxal form $h(x)$ giving weights to coords of x $\in \mathcal{X}$  
    
    - then approve credit is $\sum{_1^d} x_i w_i$ > threshold, deny if $\sum{_1^d} x_i w_i$ <  threshold  

    - more compact: $h(x) = sign((\sum{_1^d} x_i w_i) + b)$  

        - where $sign(s) = 1 \ if s >0$, 0 otherwise 
        - and $b$ is bias that determines the threshold bc credit is approved if 
        $\sum{_1^d} x_i w_i > - b$  
    - this is a perceptron. we use the data to determine weights + threshold. negative weights are for indicators bad on credit. b will dictate how lenient or stringent we are.  

$E_{in}(h)$ is fraction of $\mathcal{D}$ where f (fx) and h(ypothesis) disagree, calculated as the proportion of disagreements      

# ch2 - generalization   
generalization error = diff between $E_{in}$ and $E_{out}$, hoeffding is one way to quantify this. does so via probabilistic bound. can think of it as  
- pick a tolerance delta like .05  
- say with p = 1 - delta that 
$$E_{out}(g) \leq E_{in}(g) + \sqrt{{ \frac{1}{2N} \ ln \ \frac{2M}{\delta}}}$$

where M is the size of hypothesis set  

can think of $\sqrt{{ \frac{1}{2N} \ ln \ \frac{2M}{\delta}}}$ as some $\epsilon$ added to $E_{in}$  

- most interesting situations have unlimited hypotheses. so we need to replace our M and we can do this with $m_H$ - the maximum # of dichotomies can be generated by H on N points - in this starting place with N points and binary target fx  
    - where we end up is that any growth fx w break point is bounded by polynomial 

$$E_{out}(g) \leq[?]  \ \ E_{in}(g) + \sqrt{{ \frac{1}{2N} \ ln \ \frac{2\mathcal{m}_h}{\delta}}}$$  

- except when $d_{VC}$ is $\infty$ -- those are the class of bad models where there aren't clear generalizability conclusions  

- this actually is an oversimplification but general idea's right  

- big picture the VC gives some of the most general routes we have and is good for a loose estimate but in a real application we'll use a test set to forecast $E_{out}$ more precisely  

## bias - variance tradeoff  
- amounts to another approach to generalization; rather than bound error on E_out we'll decompose it into bias + variance 
- bias is $(\bar{g}(x) - f(x))^2$; in words the estimated fx's discrep from frue fx and the averaging is over possible data sets   

# ch3 - linear model   
- in ch1 our perceptron had the hypothesis set $\mathcal{H}$ of all lines (hyperplanes) sep'ing the points  
    - then it iteratively corrected the lines to improve $E_{in}$  

- we like lines cause they small VC dimension  

- linear mod for classification involves hyp set of linear classifiers, each h 
    $$h(x) = sign(\textbf{w}^T\textbf{x})$$
    - with $w \in \R^{d+1}$ input space tho actually considered d-dim bc term1 is a bias  

- our VC epsilon here is $\ O(\sqrt{(\frac{d}{N} \ ln \ N)})$ (see notation box)  which means that asymptotically $E_{in}$ and $E_{out}$ will be close. what about getting a small $E_{in}$ tho? depends ofc on if there exists linear hyp that makes this small. if data are linearly separable, $\exists \textbf{w}*$ where $E_{in}(\textbf{w}*) = 0$

- relaxing from linear separability, we can strive for classification solving this optimization prob:  
$$\min_{\textbf{w} \in R^{d+1}} \sum_{n=1}^N \frac{1}{N} [sign(\textbf{w}^T\textbf{x}) \neq y]$$
    - but that's hard because of the discrete nature of sign (and [ ]? - don't get this)  
    - so rather we min $E_{in}$ approx  

- we'll mod the perceptron algo by keeping a running best weight vector "in our pocket" and checking its $E_in$, replacing the one we're currently storing if it's better than the prior  
    - to do this, it evaluates all examples on a given iter (before just did a subset) so more expensive + no convergence guarantees  

## linear regr  
- moving on now to real-valued outcmoes not binary classification  
- goal in LR to min squared discr between h(x) and y where the expected error defined against the joint pdf, $p(\textbf{x},y)$  
- we want a small $E_{out}$ but don't have this so resort to $E_{in}$  
    - take a linear combo where  
    $$h(x) = \sum_{i=0}^{d} w_i x_i = \textbf{w}^T\textbf{x}$$
- as before x is 1 in first position, and numbers $\in R^d$ elsewhere (considered d-dimens bc of the bias) and the weights have the 1-higher dimensionality, $w \in R^{d+1}$  

- we optimize overall weights 
$$w_{lin} = \operatorname*{argmin}_{w \in d+1} E_{in}\textbf{w}$$

- this implies differentiability so we can find min by setting deriv to 0 
$$\nabla E_{in}(w)=\textbf{0}$$
- the gradient's a column vector whose ith column is the partial deriv of $E_in \ wrt \ w_i$  
- we can actually get the unique solution (if invertible) via
---
***LR Algo***
-  constructing a matrix of the predictors X  
- computing $X^{pseudo} = X^T(X^T X^T)^{-1}$
- then  $w_{lin}=X^{pseudo}y$
---
- this is OLS. cool bc it's just matrix mult but still learning; possible bc analytic sol'n  

## non-linear transformation  
- our linear regr's have all taken the form  
$$\sum_{i=0}^{d} w_i x_i = \textbf{w}^T\textbf{x}$$
    - linear bc of the $w_i's$ ; from the algo's perspective, the x's are just constants. so we can transform them however we want  
    - and this is often desirable bc we may not expect any predictor to linearly bear on outcome eg. years in a residence regressed on credit - is it really plausible 40 yrs is twice as trustworthy as 20?  

# lec 5 - error and noise  
    

---
***Notation Box***

*$O(\cdot)$* - is a constant multiple of the argument is larger asymptotically than the term's abs value   

---

# lec 9 
- we've ended up with idea that number of examples need is proportional to N examples needed  

## nonlinear transformation wrap up - w an eye twd generalization issues  
- nonlinear transofrmation 
$x = (x_0, x_1, ... x_d) \rightarrow^{\phi} z = (z_0, z_1 ... z_d)$ - this is z space   

## stochastic gradient descent for logistic regression  
- using the taylor series approximation where we ignore all h.o.t. above the linear. we take a small step size $\eta$ because otherwise we might miss important info in the step from the h.o. contributions. we take the linear weight change times a unit vector $\hat{v}$ that just encodes the direction of the change. this gives us the change in $E_{in}$. we apply the neg so can minimize  

$$\Delta E_{in} = -\eta\nabla E_{in}\textbf{w}(0) \hat{v}$$ 

so we've an inner product between the weight and a unit vector. so abs most we'll get norm * weight vec (or neg * weight vec if the unit's negative). so we use the norm  

$$ \ \ \ \\  \ \ \ \  \  \   \geq -\eta||\nabla E_{in}\textbf{w}(0) \hat{v}||$$

and thus bc $\hat{v}$ is a unit vector  
$$\hat{v} = \frac{\Delta E_{in}(\textbf{w}(0))}  {||\Delta E_{in}(\textbf{w}(0))||}$$ 

# lec 10 - neural nets  
- we'll do gradient descent in batches by using one example  

- stochastic gradient descent (sgd) where we use an average. why?  
    
    - 1. cheaper computation bc we just need one example  
    
    - 2. randomization, which can be good when get stuck in local min  

    - 3. simple - it's the simplest optimization you can think of  

    
    